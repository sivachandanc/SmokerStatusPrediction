{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/31 18:30:04 WARN Utils: Your hostname, sivachandan resolves to a loopback address: 127.0.1.1; using 10.0.0.187 instead (on interface wlp8s0)\n",
      "23/01/31 18:30:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/31 18:30:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark_session = SparkSession.builder.appName('Siva').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+---------+--------------+---------------+-------------+--------------+--------+----------+-------------------+-----------+------------+---+---+----------+-------------+----------------+----+----+---+-------------+-------+\n",
      "|age|height(cm)|weight(kg)|waist(cm)|eyesight(left)|eyesight(right)|hearing(left)|hearing(right)|systolic|relaxation|fasting blood sugar|Cholesterol|triglyceride|HDL|LDL|hemoglobin|Urine protein|serum creatinine|AST |ALT |Gtp|dental caries|smoking|\n",
      "+---+----------+----------+---------+--------------+---------------+-------------+--------------+--------+----------+-------------------+-----------+------------+---+---+----------+-------------+----------------+----+----+---+-------------+-------+\n",
      "|35 |170       |85        |97.0     |0.9           |0.9            |1            |1             |118     |78        |97                 |239        |153         |70 |142|19.8      |1            |1.0             |61  |115 |125|1            |1      |\n",
      "|20 |175       |110       |110.0    |0.7           |0.9            |1            |1             |119     |79        |88                 |211        |128         |71 |114|15.9      |1            |1.1             |19  |25  |30 |1            |0      |\n",
      "|45 |155       |65        |86.0     |0.9           |0.9            |1            |1             |110     |80        |80                 |193        |120         |57 |112|13.7      |3            |0.6             |1090|1400|276|0            |0      |\n",
      "|45 |165       |80        |94.0     |0.8           |0.7            |1            |1             |158     |88        |249                |210        |366         |46 |91 |16.9      |1            |0.9             |32  |36  |36 |0            |0      |\n",
      "|20 |165       |60        |81.0     |1.5           |0.1            |1            |1             |109     |64        |100                |179        |200         |47 |92 |14.9      |1            |1.2             |26  |28  |15 |0            |0      |\n",
      "+---+----------+----------+---------+--------------+---------------+-------------+--------------+--------+----------+-------------------+-----------+------------+---+---+----------+-------------+----------------+----+----+---+-------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw = spark_session.read.csv('./data/train_dataset.csv',header=True)\n",
    "df_raw.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df_raw.select([col(c).cast('float').alias(c) for c in df_raw.columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df_transformed.select"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smoke",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "03ac68f638dae92304f2f2e247e788c8032106c1fa438a2f08e4fd986f62f3e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
